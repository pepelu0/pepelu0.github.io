---
layout: post
title: 【翻译】多线程IO实验版Redis基准测试
author: pepelu
date: 2019-09-01
online: true
tags:
    - redis
---

***

原文地址: [Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314?gi=b87fffac2b2e)

原作者: [Filipe Oliveira](https://www.linkedin.com/in/filipecosta90/)，Performance Engineer @ Redis Labs

***

人们普片觉得Redis是单进程，单线程模式的应用。但是对于新版本的Redis来说，这种想法显然已经过时了。

引用[Redis官方文档](https://redis.io/topics/faq)原文：

> 从Redis4.0开始，我们开始让Redis使用更多的线程。当前多线程仅应用在后台删除对象，阻塞Redis模块中实现的命令。下一个版本，我们计划引入更多的多线程处理。

Redis不是单线程的，它在后台跑了很多线程，用来做清理工作，包括清理脏数据，关闭文件描述符FD等等。Redis也不是单进程的，因为在后台保存数据的时候，Redis也会fork子进程。

让我们设想一个最简单的方式来让Redis更加多线程化：每当Redis需要处理读、写操作的时候，我们都调度N个空闲的IO线程来处理。由于我们还是用主线程来响应大部分任务，这么处理并不会增加太多的复杂性。反倒是由于Redis大部分的时间都用来处理IO了，我们能得到更多的好处，我们需要认识到的是：Redis是内存或者网络密集型的。

<br/>

# 虚拟机调优 & 找到Redis代码中的CPU热点

   为了找到Redis究竟花了多少时间在IO上，我们在GCP上申请了两台**n1-highcpu-96**实例，其中一台作跑redis-server(多线程IO版本)，另一台跑redis-benchmark。

   为了让redis-server能跑的更效率写，我们调优了实例上的某些参数：

   * 禁用**tuned**和**ktune**电量节省相关机制
   * 启用**sysctl**设置，以加速磁盘和网络IO，并使用**deadline scheduler**
   * 设置[CPU调节器](https://en.wikipedia.org/wiki/CPU_governor)为 `performance` 级别
   * 禁用透明大页(Transparent Huge Pages)
   * 调整最大监听队列队列长度(somaxconn)为65535【译者注：默认为1024】
   * 设置vm.overcommit_memory参数，从0(默认值)改为1(从不拒绝任何的malloc)

   以上修改可以用下面的命令完成。最后的一行命令确保了sysctl设置能够立即生效。

   ```bash
   sudo -i
   echo never > /sys/kernel/mm/transparent_hugepage/enabled
   sysctl -w vm.overcommit_memory=1
   sysctl -w net.core.somaxconn=65535
   sysctl -p
   ```

## 单线程redis-server性能分析

   1. redis-server虚拟机

      我们已经把虚拟机实例调优完了，可以用下面的配置启动redis-server了

      ```bash
      fcosta_oliveira@n1-highcpu-96-redis-server-1:/redis-server --protected-mode no --save "" --appendonly no   --daemonize yes
      ```

   2. redis-benchmark虚拟机

      为了衡量Redis的性能，产生足以分析Redis性能的负载，我们在这篇文章中大量使用了redis-benchmark。官方的redis-benchmark是生成测试结果快速而有效的方式。

      我们从官方的redis-benchmark中fork了一个[版本](https://github.com/filipecosta90/redis/tree/benchmark_xadd)，并增加了一些新的针对Streams数据类型的测试。我们已经提交了[PR](https://github.com/antirez/redis/pull/6015)。

      这部分的讨论中，我们只想知道Redis花了多少时间在IO上，以用来根据阿姆达尔定律【译者注：[阿姆达尔定律](https://en.wikipedia.org/wiki/Amdahl%27s_law)为并行计算系统的设计者提供了理论指导】来预测使用并行处理的理论速度增长。
      为了达到这个目的，我们使用新的多线程redis-benchmark，创建15个客户端，来发送10,000,000个键大小为100字节的GET命令。

      ```bash
      fcosta_oliveira@n1-highcpu-96-redis-benchmark-1:~/redis-benchmark -t get -c 150 -n 10000000 — threads 46 -h {ip of redis-server vm} -d 100
      ```

   3. 跑基准测试时的栈调用分析

      在**n1-highcpu-96-redis-benchmark-1**虚拟机上跑基准测试的同时，我们用Linux的perf_events(也叫"perf")命令在redis-server所在的虚拟机上以99Hz的抽样率进行栈抽样。

      ```bash
      fcosta_oliveira@n1-highcpu-96-redis-server-1:~/sudo perf record -F 99 — pid `pgrep redis-server` -g -o 100_bytes_no_iothreads
      ```

      结果长这样：

      ![redis_stack_trace_profile](/assets/redis_stack_trace_profile.png)

      在使用多线程IO实验版Redis提升性能之前，redis-server的执行时间如上图所示。这其中包括了会提升的部分和不会提升的部分。根据上图所示的情况，有46.6%的执行时间(sys_write)是有可能被多线程IO实验版Redis提高的。

      性能提升的上限，是程序中哪些还在串行的部分。在我们的例子中是53.4%。也就是说，理论上，我们最多提升2倍的性能，也就是下图中蓝线部分。

      ![Amdahl's Law](/assets/amdahl_law.png)

<br/>

# 硬件上我们能达到的上限是多少？

   1. 网络上限

      网络带宽和延迟通常会对Redis性能产生直接的影响。在深入我们的基准测试之前，我们先来用qperf命令检查一下跑基准测试和redis-server的两台虚机之前的网络延迟。

      下面的实验结果，是在单租户，无竞争的网络环境中，在这样的网络环境中，没有后台的其他程序使用网络，因此也不存在网络干扰。
      
      ```bash
      fcosta_oliveira@n1-highcpu-96-redis-benchmark-1:~/qperf$ qperf -t 60 -v {ip of redis-server vm} tcp_bw 
      tcp_lattcp_bw:    
         bw              =  3.05 GB/sec    
         msg_rate        =  46.5 K/sec    
         time            =    60 sec    
         send_cost       =   248 ms/GB    
         recv_cost       =   181 ms/GB    
         send_cpus_used  =  75.6 % cpus    
         recv_cpus_used  =  55.3 %
      cpustcp_lat:    
          latency        =  27.6 us    
          msg_rate       =  36.3 K/sec    
          time           =    60 sec    
          loc_cpus_used  =  14.6 % cpus    
          rem_cpus_used  =  15.9 % cpus
      ```

      我们可以看出，网络带宽是3.05GB/s(24.4Gbit/s)。之前的基准测试以100 Bytes作为Redis键，可以计算出，网络上限是每秒32,000,000次调用。
      
   2. 内存上线
   
      让我们先来看看Redis官方文档

      > RAM速度和内存带宽似乎对整体性能没有什么影响，尤其是对小对象来说。对于大对象(>10KB)，有可能观察到影响

      尽管如此，我们还是会用[STREAM](https://redis.io/topics/streams-intro)对虚拟机内存带宽做基准测试。
      
      下面的命令会获取一个STREAM，并设置一个大于n1-highcpu-96缓存大小的数组。我们重复测试十次，并剔除第一次的结果。
      
      ```bash
      git clone https://github.com/jeffhammond/STREAM.git
      cd STREAM/
      gcc -fopenmp -D_OPENMP -O -DSTREAM_ARRAY_SIZE=100000000 stream.c -o stream.100M
      export OMP_NUM_THREADS=48
      ./stream.100M
      ```
      
      第二次的结果如下:
      
      ```
      fcosta_oliveira@n1-highcpu-96-redis-server-1:~/STREAM$ ./stream.100M
      -------------------------------------------------------------
      STREAM version $Revision: 5.10 $
      -------------------------------------------------------------
      This system uses 8 bytes per array element.
      -------------------------------------------------------------
      Array size = 100000000 (elements), Offset = 0 (elements)
      Memory per array = 762.9 MiB (= 0.7 GiB).
      Total memory required = 2288.8 MiB (= 2.2 GiB).
      Each kernel will be executed 10 times.
       The *best* time for each kernel (excluding the first iteration)
       will be used to compute the reported bandwidth.
      -------------------------------------------------------------
      Number of Threads requested = 48
      Number of Threads counted = 48
      -------------------------------------------------------------
      Your clock granularity/precision appears to be 1 microseconds.
      Each test below will take on the order of 15223 microseconds.
         (= 15223 clock ticks)
      Increase the size of the arrays if this shows that
      you are not getting at least 20 clock ticks per test.
      -------------------------------------------------------------
      WARNING -- The above is only a rough guideline.
      For best results, please be sure you know the
      precision of your system timer.
      -------------------------------------------------------------
      Function    Best Rate MB/s  Avg time     Min time     Max time
      Copy:           83936.4     0.020901     0.019062     0.022618
      Scale:          83927.0     0.022181     0.019064     0.034278
      Add:            93240.4     0.027765     0.025740     0.031065
      Triad:          93959.3     0.027276     0.025543     0.030629
      -------------------------------------------------------------
      Solution Validates: avg error less than 1.000000e-13 on all three arrays
      -------------------------------------------------------------
      ```
      
      我们可以观察到，Copy最高能达到83GB/s。那么在我们的例子中，一个100字节大小的键能最多获得每秒890,000,000次查询。我们并不是要在这里讨论不同的内存访问方式 - 
      我们测量出的最佳速率和硬上限，可以帮助我们粗略的进行估算。
      
<br/>

# 用不同的配置跑基准测试

   我们的实验将从面向内存的，进行内存到内存的网络IO的操作开始研究起，这些操作包括了：

   * SET，生成Redis写操作。写操作的时间复杂度为O(1)。
   * GET，生成Redis读操作。读操作的时间复杂度为O(1)。
   * MSET，生成Redis写操作，该写操作的时间复杂度为O(N)，N是要写入的键值的数量。
   * HSET，生成Redis写操作，写入更加复杂的数据结构，时间复杂度为O(1)。
   * LRANGE前100个元素，生成Redis读操作，该操作的时间复杂度为O(S+N)，取决于list(S)的长度，以及我们想要获取的元素的个数(N)。
   * X_ADD，在一个只允许追加的数据结构上生成Redis写操作。该操作的时间复杂度为O(N)。我们将会扩展这个测试到一些字段值的组合上：STTREAM上的一个字段值的操作XADD_1，5个字段值组合XADD_5，10个字段值组合XADD_10。

   所有的操作都会配置为生成三种不同大小的对象：3 Bytes，100 Bytes，1 KB。为了减少基准测试中的人工成本，我们写了下面的脚本来生成对象。然而，我们还是需要按照需求来修改redis-server的io-threads配置。接下来一个部分我们将展示要在**n1-highcpu-96-redis-server-1**和**n1-highcpu-96-redis-benchmark-1**上的指令顺序。

   ```bash
   #!/bin/bash

   # script to run redis benchmark with different payload, pipeline, and iothreads ( the iothreads are configured on the redis-server VM
   # and that argument is only for creating a different file for different redis-server configs

   args=("$@")
   n=${args[0]}
   h=${args[1]}
   iothreads=${args[2]}
   dir=${args[3]}

   echo "Saving results in " $dir;

   for connections in  512; do
   	for payload_size in  3 100 1000; do
   	for pipeline_size in  1 5 10; do
   	echo "running for pipeline size" $pipeline_size ", payload size of" $payload_size ", connections" + $connections;
   		date;
   		redis-benchmark -t mset,set,get,hset,lrange_100,xadd -c $connections -P $pipeline_size -n $n --threads 48 -h $h -d $payload_size >$dir/"iothreads_"$iothreads"_pipeline_"$pipeline_size"_connections_"$connections"_payload_"$payload_size;
   	done;
   	done;
   done;
   ```

   1. 在基准测试机器上：

      ```bash
      @n1-highcpu-96-redis-benchmark-1:/mkdir results
      ```

   2. 配置0个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 1
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh “1000000” “10.168.0.2” “0” “./results”
      ```

   3. 配置8个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 8
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh “1000000” “10.168.0.2” “8” “./results”
      ```

   4. 配置16个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 16
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh”1000000" “10.168.0.2” “16” “./results”
      ```

   5. 配置24个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 24
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh “1000000” “10.168.0.2” “24” “./results”
      ```

   6. 配置32个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 32
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh “1000000” “10.168.0.2” “32” “./results”
      ```

   7. 配置48个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 48
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh “1000000” “10.168.0.2” “48” “./results”
      ```

   8. 配置64个io线程

      * 在redis-server上：

      ```
      @n1-highcpu-96-redis-server-1:/redis-server — protected-mode no — save “” — appendonly no — io-threads 64
      ```

      * 在基准测试机器上：

      ```
      @n1-highcpu-96-redis-benchmark-1:/./run-redis-benchmark.sh”1000000" “10.168.0.2” “64” “./results”
      ```

<br/>

# 读写速率的确增加了

   我们发现用16个或者用24个io线程(取决于命令类型)可以大幅提高性能。比如，用24个io线程可以把GET请求从每秒173K个请求提升到每秒285K个，涨了每秒110K，也就是1.6倍的增速。

   很容易看到，随着消息大小的增长，吞吐量，也就是每分钟请求量，减小了。可以看到，所有命令都是这个样子。

   我们决定分开跑Redis STREAMS基准测试。STREAMS的测试结果和其他Redis操作差不多，8个io线程可以把每秒请求数从117K提升到210K。

   下面的图表展示了单个命令情况下的 & 5个和10个命令使用pipelines情况下测量的每秒请求数。

## 性能测试

   1. 单个命令的性能(没用pipeline)

      ![single_command_performance](/assets/single_command_performance.png)

   2. 用Redis STREAMS单个命令性能(没用pipeline)

      ![streams_single_command_performance](/assets/streams_single_command_performance.png)

   3. 5个命令组成的pipeline的性能

      ![5_command_pipeline_performance](/assets/5_commands_pipeline_performance.png)

   4. 用Redis STREAMS的5个命令pipeline性能

      ![streams_5_commands_pipeline_performance](/assets/streams_5_commands_pipeline_performance.png)

   5. 10个命令组成的pipeline性能

      ![10_commands_pipeline_performance](/assets/10_commands_pipeline_performance.png)

   6. 用Redis STREAMS的10个命令pipeline性能

      ![streams_10_commands_pipeline_performance](/assets/streams_10_commands_pipeline_performance.png)

## 使用多线程IO实验室版Redis的调用栈性能分析

   如果你也在**n1-highcpu-96-redis-server-1**虚机上使用Linux perf_events(也叫"perf")命令来对16个io线程的SET命令进行性能分析的话，应该会得出如下图相似的结论。写FD的时间从45%降低到了5%。

   ![multi_thread_io_redis_stack_trace_profile.png](/assets/multi_thread_io_redis_stack_trace_profile.png)

<br/>

# 结论

   我们已经对多线程读、写操作做了基准测试。然而，这些io线程还能被用作命令执行和其他的非CPU密集型操作。

   引用[Salvatore](https://twitter.com/antirez/status/1117308774459555841)的话：

   > Work is continuing to put request parsing in the same code path.

   多线程天然会比单线程有更高的性能上限，但是在当今的云计算环境的挑战下，单机的上限已经不能满足我们了。

   ![server_speed_adoption](/assets/server_speed_adoption.png)

   感谢新标准下的40Gbps和100Gbps网卡已经被大部分云提供商(AWS使用的[Elastic Network Adapter](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html)，Azure上所使用的100Gbps的[ExpressRoute Direct](https://azure.microsoft.com/en-us/blog/azure-networking-fall-2018-update/)和[HB and HC Azure VMs](https://azure.microsoft.com/en-us/blog/introducing-the-new-hb-and-hc-azure-vm-sizes-for-hpc/)，还有[32Gbps for GCP](https://cloud.google.com/vpc/docs/quota#per_instance))所应用，
   网络带宽，作为第一个瓶颈，已经渐渐不存在了。因此，我们现在需要关心的重点是，如何更大限度的利用多核的计算优势以及网卡的速度优势。

   我们接下来会继续探索多线程技术在多实例的集群方案中可以如何优化性能。在下一篇文章中，我们会对此进行深度探索。

   根据之前的分析，用100 Bytes键大小的话，网络传输上限是每台机器每秒32,000,000次查询。就算使用1000 Bytes键大小，网络传输上限也能达到每秒3,000,000次。为了打满网络，我们需要16台独立的redis服务，每台承受每秒200,000次请求。

   Redis Labs已经创造了在1ms延迟下，使用26个AWS上的EC2节点的[每秒50,000,000](https://redislabs.com/docs/linear-scaling-benchmark-50m-ops-sec/)的记录，也就是每实例每秒2,000,000次请求。然而，如果使用AWS的[C5n Instances designed for compute-heavy applications](https://aws.amazon.com/cn/blogs/aws/new-c5n-instances-with-100-gbps-networking/)，再使用100 Gbps的更高发包速率的网络，我们能还可以用更少的虚拟机达到每秒50,000,000次请求，且延迟都小于1ms。这为今后开源Redis版本打破Redis Labs记录打下了坚实的基础。
